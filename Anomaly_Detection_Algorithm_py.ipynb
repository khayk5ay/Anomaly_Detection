{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTkExJOphDn9iOCr7jyJsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khayk5ay/Anomaly_Detection/blob/main/Anomaly_Detection_Algorithm_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The essence of an anomaly detection algorithm is to detect strange occurences whose probability of occurence is quite unlikely."
      ],
      "metadata": {
        "id": "KW9KFMrw2RiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaiRu9VQ2Ird"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random dataset for the algorithm analysis\n",
        "def generate_dataset():\n",
        "  X_train_list = []\n",
        "  X_val_list = []\n",
        "  y_val_list = []\n",
        "\n",
        "  # Generate the training set containing values considered to be normal\n",
        "  for i in range(50):\n",
        "    X_train_list.append([random.uniform(25,30), random.uniform(3,5)])\n",
        "\n",
        "  X_train = np.array(X_train_list)\n",
        "\n",
        "  # Generate the validation set of some normal and other anomolous values\n",
        "  for i in range(20):\n",
        "    X_val_list.append([random.uniform(25,30), random.uniform(3,5)])\n",
        "    y_val_list.append(0)\n",
        "  for i in range(5):\n",
        "    X_val_list.append([random.uniform(19,27), random.uniform(2,7)])\n",
        "    y_val_list.append(1)\n",
        "  for i in range(5):\n",
        "    X_val_list.append([random.uniform(28,35), random.uniform(2,7)])\n",
        "    y_val_list.append(1)\n",
        "\n",
        "  X_val = np.array(X_val_list)\n",
        "  y_val = np.array(y_val_list)\n",
        "\n",
        "  return X_train, X_val, y_val"
      ],
      "metadata": {
        "id": "sfdbOlCBcJ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise Training dataset for training\n",
        "X_train, X_val, y_val = generate_dataset()\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "ox-hWlsY6yhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the data\n",
        "plt.scatter(X_train[:,0], X_train[:, 1], marker='o')\n",
        "plt.scatter(X_val[:,0], X_val[:, 1], color='r', marker='x')\n",
        "plt.ylim(0,8)\n",
        "plt.xlim(10,40)"
      ],
      "metadata": {
        "id": "i2D9PRvnUPBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the distribution of each of the features in X_train\n",
        "\n",
        "fig, ax = plt.subplots(2,1, figsize=(10,10))\n",
        " \n",
        "ax[0].hist(X_train[:,0], bins=5) \n",
        "ax[1].hist(X_train[:,1], bins=3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lpvzFUC_BN8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the data does not necessarily have a normal / gaussian distribution, the algorithm still does well to detect any anomalies"
      ],
      "metadata": {
        "id": "Y7W0RMk5xzZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the values if Mean and Variance for the data set\n",
        "def get_gaussian_distribution(X):\n",
        "  \"\"\"\n",
        "  gets the gaussian distribution of the data\n",
        "\n",
        "  parameters:\n",
        "  X (numpy ndarray) : m * n dimensioned unlabeled data\n",
        "\n",
        "  returns:\n",
        "  mu () : 1 * n array showing the mean of the data\n",
        "  var () : 1 * n array showing the variance of the data\n",
        "  \n",
        "  \"\"\"\n",
        "  m, n = X.shape\n",
        "  # Compute the mean of each featrue in the data set\n",
        "  mu = sum(X) / m\n",
        "  # Compute the variance of each feature in the data set\n",
        "  var = sum((X-mu)**2) / m\n",
        "  \n",
        "  return mu, var"
      ],
      "metadata": {
        "id": "WF179sSd3CIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu, var = get_gaussian_distribution(X_train)"
      ],
      "metadata": {
        "id": "1Tzt65GV-2Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The mean is {mu}\")\n",
        "print(f\"The variance is {var}\")"
      ],
      "metadata": {
        "id": "F_r_S8QRSk6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ p(x ; \\mu,\\sigma ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x - \\mu)^2}{2 \\sigma ^2} }$$"
      ],
      "metadata": {
        "id": "pnGUMEG6GIor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get probabilities of features when provided with the array of all observations when provided witht he values of the gaussian distribution \n",
        "\n",
        "def get_probabilities(X, mu, var):\n",
        "  # Initialise the array to hold the probabilities\n",
        "  p_x_j = np.zeros(len(X))\n",
        "  \n",
        "  # Get probabiliities for each observation\n",
        "  for j in range(len(X)):\n",
        "    # Initialise the array that will hold the probabilities for each feature associated with the overall observation\n",
        "    p_x_i = np.zeros(X[j].shape[0])\n",
        "    for i in range(X[j].shape[0]):\n",
        "      # Compute the probability of each individual feature\n",
        "      denom = np.sqrt(2 * np.pi * var[i])\n",
        "      exp_val = -((X[j][i]-mu[i]) ** 2 )/ (2 * var[i])\n",
        "      p_x_i[i] = (1 / denom) * np.exp(exp_val)\n",
        "\n",
        "    # Compute the overall probability of that observation as the product of all the feature probabilities\n",
        "    p_x_j[j] = np.prod(p_x_i)\n",
        "\n",
        "  return p_x_j"
      ],
      "metadata": {
        "id": "c9MEes5KGHSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_probabilities(X_train, mu, var)\n",
        "#len(X_train[0].shape)"
      ],
      "metadata": {
        "id": "tius3XE_ILts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the threshold probability below which an observation will be considered abnormal\n",
        "# The threshold will be considered using the F1 score\n",
        "def select_threshold(y_val, p_val):\n",
        "\n",
        "  best_F1 = 0\n",
        "  best_epsilon = 0\n",
        "  step_value = (p_val.max() - p_val.min()) / 1000\n",
        "  # Consider a wide range of theshold values\n",
        "  for epsilon in np.arange(p_val.min(), p_val.max(), step_value):\n",
        "    \n",
        "    predictions = p_val < epsilon\n",
        "    # Compute the True Positive (tp), False Positive(fp), False Negative(fn)\n",
        "    tp = sum(predictions[y_val == 1])\n",
        "    fp = sum(predictions[y_val == 0])\n",
        "    fn = sum(y_val[predictions == 0])\n",
        "\n",
        "    precision_score = tp / (tp + fp)\n",
        "    recall_score = tp / (tp + fn)\n",
        "    \n",
        "    #Compute the f1 score for each value of epsilon\n",
        "    F1 = (2 * precision_score * recall_score) / (precision_score + recall_score)\n",
        "\n",
        "    if F1 > best_F1:\n",
        "      best_F1 = F1\n",
        "      best_epsilon = epsilon\n",
        "\n",
        "  return best_F1, best_epsilon    \n"
      ],
      "metadata": {
        "id": "NL1pH0cPL3zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_probabilities(X_val, mu, var)"
      ],
      "metadata": {
        "id": "ASZbzvpewOiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F1_score, epsilon = select_threshold(y_val, get_probabilities(X_val, mu, var))\n",
        "print(\"F1 Score \", F1_score)\n",
        "print(\"Best Epsilon \", epsilon)"
      ],
      "metadata": {
        "id": "L3fC3io1joL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXLWrvfXuy0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}